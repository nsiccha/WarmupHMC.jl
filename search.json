[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adaptive warm-up for linear and non-linear transformations",
    "section": "",
    "text": "A previous approach to do automatic non-linear reparametrizations using MCMC draws has been found lacking, AFAICT mainly for two reasons:"
  },
  {
    "objectID": "index.html#method-code",
    "href": "index.html#method-code",
    "title": "Adaptive warm-up for linear and non-linear transformations",
    "section": "Method & code",
    "text": "Method & code\n\nMethod\nWorking on an explanation, but see also the commented code below which includes an explanation in the docstring at the top. The docstring is also reproduced below:\nAdaptively\n\nlearn a (currently only) linear transformation of the posterior that simplifies MCMC sampling,\nlearn a NUTS step size (using standard Dual Averaging), and\nreturn samples from the posterior.\n\nThe warm-up procedure is windowed and insipred by Stan’s and nutpie’s warm-up procedures, but differs in several important ways:\n\nWe initialize using Pathfinder!\nOur warm-up windows aim to reach a certain number of GRADIENT EVALUATIONS , instead of a certain number of MCMC transitions (Stan). We start with a (default) target of 1000 gradient evaluations, and double that target after each warm-up window.\nInstead of only using the posterior positions (Stan), we use the posterior POSITIONS AND GRADIENTS (like e.g. nutpie).\nInstead of only using the MCMC/posterior positions and gradients (Stan and nutpie), we also store and use the INTERMEDIATE POSITIONS AND GRADIENTS, i.e. the ones that MCMC visits before returning the “final” new position. We store up to (a default of) 1000 intermediate positions and gradients. The stored intermediate positions get selected (pseudo-)randomly, and only get selected if the Hamiltonian error is small enough.\nInstead of only learning a single (linear) transformation and upating that one repeatedly, we learn several transformations in parallel and at the end of each warm-up window select the one that minimizes a loss function. Currently, we learn three different linear transformations:\n\nPathfinder’s initial transformation, enriched by an updated additional diagonal scaling,\nA standard diagonal “mass matrix”.\nA novel, adaptive sequence of Householder transformations followed by diagonal scaling.\n\nThe loss function that gets used to select the used linear transformation tries to reward transformations which turn the transformed posterior into something that’s close to a Normal distribution without correlation. For transformed positions p’ and gradients g’, the loss function is loss(p', g') = sum(log(std(p') * std(g'))^2), which is zero for Normal distributions with zero correlations, independently of the sampled positions and gradients.\nInstead of running the warm-up for a fixed number of windows, we try to estimate when continuing warming up is harmful/useless and stop warming up then. To facilitate this, we\n\nonly ever adapt the stepsize until (a default of) 50 MCMC transitions have ocurred in the current warm-up window,\nand try to predict cost (in terms of gradient evaluations) for finishing sampling with the current kernel vs. restarting warm-up with a new window. The way we do this prediction will probably be changed in the future.\n\nBy only ever having 50 stepsize adaptation MCMC transitions, we can start collecting posterior samples early.\n\n\n\nCode: adaptive_warmup_mcmc.jl\n\nThe main code can be found at https://github.com/nsiccha/WarmupHMC.jl/blob/main/src/adaptive_warmup_mcmc.jl, also reproduced below:\n\"\"\"\nAdaptively \n\n* learn a (currently only) linear transformation of the posterior that simplifies MCMC sampling,\n* learn a NUTS step size (using standard Dual Averaging), and\n* return samples from the posterior. \n\nThe warm-up procedure is windowed and insipred by [Stan](https://mc-stan.org/docs/reference-manual/mcmc.html#automatic-parameter-tuning)'s and [nutpie](https://github.com/pymc-devs/nutpie)'s warm-up procedures, but differs in several important ways:\n\n* We initialize using Pathfinder!\n* Our warm-up windows aim to reach a certain number of GRADIENT EVALUATIONS , instead of a certain number of MCMC transitions (Stan).\nWe start with a (default) target of 1000 gradient evaluations, and double that target after each warm-up window.\n* Instead of only using the posterior positions (Stan), we use the posterior POSITIONS AND GRADIENTS (like e.g. nutpie).\n* Instead of only using the MCMC/posterior positions and gradients (Stan and nutpie), \nwe also store and use the INTERMEDIATE POSITIONS AND GRADIENTS, i.e. the ones that MCMC visits before returning the \"final\" new position. \nWe store up to (a default of) 1000 intermediate positions and gradients. \nThe stored intermediate positions get selected (pseudo-)randomly, and only get selected if the Hamiltonian error is small enough.\n* Instead of only learning a single (linear) transformation and upating that one repeatedly, we learn several transformations in parallel and\nat the end of each warm-up window select the one that minimizes a loss function. Currently, we learn three different linear transformations:\n    * Pathfinder's initial transformation, enriched by an updated additional diagonal scaling,\n    * A standard diagonal \"mass matrix\".\n    * A novel, adaptive sequence of Householder transformations followed by diagonal scaling.\n\n  The loss function that gets used to select the used linear transformation tries to reward transformations which turn the transformed posterior\ninto something that's close to a Normal distribution without correlation. \nFor transformed positions p' and gradients g', the loss function is \n    `loss(p', g') = sum(log(std(p') * std(g'))^2)`, \nwhich is zero for Normal distributions with zero correlations, independently of the sampled positions and gradients. \n* Instead of running the warm-up for a fixed number of windows, \nwe try to estimate when continuing warming up is harmful/useless and stop warming up then. To facilitate this, we\n    * only ever adapt the stepsize until (a default of) 50 MCMC transitions have ocurred in the current warm-up window,\n    * and try to predict cost (in terms of gradient evaluations) for finishing sampling with the current kernel vs. restarting warm-up with a new window.\n    The way we do this prediction will probably be changed in the future.\n\n  By only ever having 50 stepsize adaptation MCMC transitions, we can start collecting posterior samples early.\n\"\"\"\nadaptive_warmup_mcmc(\n    rng, lpdf; \n    # The number of posterior draws \n    n_draws=1000, \n    # The number of GRADIENT EVALUATIONS in the first window\n    n_evaluations=1000, \n    # The upper limit of (intermediate) positions and gradients that will be recorded and then used for adaptation\n    recording_target=1000,\n    # The maximum number of transitions (per window) for which the stepsize gets adapted \n    stepsize_adaptation_limit=50, \n    target_acceptance_rate=.8, \n    max_tree_depth=10,\n    stepsize_search=DynamicHMC.InitialStepsizeSearch(),\n    init=missing, \n    report=true, \n    monitor_ess=report,\n    ) = begin \n    # Standard Dual Averaging\n    stepsize_adaptation = DynamicHMC.DualAveraging(δ=target_acceptance_rate)\n    # Standard NUTS\n    algorithm = DynamicHMC.NUTS(;max_depth=max_tree_depth)\n    # The dimension of the posterior\n    dimension = LogDensityProblems.dimension(lpdf)\n    # A thin wrapper around the posterior that enables us to record the intermediate positions and gradients\n    recorder = LimitedRecorder2(\n        # As above\n        recording_target,\n        # The initial \"thinning\" of intermediate positions and gradients \n        n_evaluations ÷ recording_target, \n    )\n    recording_lpdf = RecordingPosterior2(lpdf; recorder)\n    # Use Stan's initialization procedure if no initial position is given\n    ismissing(init) && (init = rand(rng, Uniform(-2,+2), dimension))\n    # Run pathfinder once to get a linear transformation and a better initial position\n    pathfinder_result = pathfinder(lpdf; rng, ndraws=1, init)\n    # The better initial position\n    position = collect(pathfinder_result.draws[:, 1])::Vector{Float64}\n    # Pathfinder's linear transformation\n    pathfinder_transformation = factorize(pathfinder_result.fit_distribution.Σ).L\n    # We currently learn three linear transformation options\n    scale_options = (\n        # Corresponds to a standard diagonal mass matrix\n        Diagonal(sqrt.(diag(pathfinder_result.fit_distribution.Σ))::Vector{Float64}),\n        # Corresponds to Pathfinder's linear transformation with an added diagonal scaling term that can be updated\n        MatrixFactorization(pathfinder_transformation, Diagonal(ones(dimension))),\n        # Something new. Corresponds to a sequence of Householder reflections, followed by a diagonal scaling term. \n        # Both the reflections and the diagonal scaling term will be updated. \n        MatrixFactorization(SuccessiveReflections(dimension), Diagonal(ones(dimension)))\n    )\n    # This is needed to make DynamicHMC \"accept\" our linear transformations\n    energy_options = map(scale_options) do L\n        DynamicHMC.GaussianKineticEnergy(MatrixFactorization(L, L'), MatrixInverse(L'))\n    end\n    # At the beginning, we will use Pathfinder's transformation.\n    # At later stages of warm-up, the estimated losses corresponding to each transformation will be written to `transformation_losses` \n    transformation_losses = (0., 0., 0.)\n    active_option = 2 # Pathfinder\n    kinetic_energy = energy_options[active_option]\n    # The below tries to find a good initial stepsize. \n    position_and_gradient = DynamicHMC.evaluate_ℓ(lpdf, position; strict=true)\n    position_gradient_and_momentum = DynamicHMC.PhasePoint(position_and_gradient, DynamicHMC.rand_p(rng, kinetic_energy))\n    stepsize = DynamicHMC.find_initial_stepsize(\n        stepsize_search, \n        DynamicHMC.local_log_acceptance_ratio(\n            DynamicHMC.Hamiltonian(kinetic_energy, lpdf), position_gradient_and_momentum\n        )\n    )\n    # The below variables give us access to the matrices into which the intermediate positions and gradients and the MCMC positions an gradients will be written\n    (;\n        # Intermediate positions\n        halo_position, \n        # Intermediate gradients\n        halo_gradient, \n        # MCMC positions\n        posterior_position,\n        # MCMC gradients\n        posterior_gradient\n    ) = recording_lpdf\n    # This will try to predict the future number of steps needed, if we adopt the new linear transformation.\n    # It's currently doing something slightly silly, and I will change what exactly it does. \n    depth_predictor = DepthPredictor(max_tree_depth)\n    # For monitoring purposes: Keep track of the number of gradient evaluations during warm-up\n    total_evaluation_counter = 0\n    # For monitoring purposes: Keep track of the number of warm-up windows so far\n    outer_counter = 0\n    # For monitoring purposes: Keep track of the number of the total number of MCMC transitions\n    total_transition_counter = 0\n    # For monitoring purposes: Displays the progress and additional info\n    progress = report ? ProgressMeter.Progress(n_draws; dt=1e-3, desc=\"Sampling...\") : nothing\n    # For monitoring purposes: Keep track of the minimal effective sample size so far\n    min_ess = missing\n    # For monitoring purposes: Keep track of the current number of gradient evaluations per MCMC transition\n    steps_per_draw = missing\n    # For monitoring purposes: Keep track of the number of divergences in the current WARM-UP window\n    n_divergent = missing\n    # For monitoring purposes: Keep track of the number of divergences in the current SAMPLING window\n    n_divergent_samples = missing\n    # We run the warm-up procedure until we have collected enough samples\n    while size(posterior_position, 2) &lt; n_draws\n        # Some setup that has to happen at the beginning of every warm-up window\n        outer_counter += 1\n        hamiltonian = DynamicHMC.Hamiltonian(kinetic_energy, recording_lpdf)\n        stepsize_state = DynamicHMC.initial_adaptation_state(stepsize_adaptation, stepsize)\n        stepsize = DynamicHMC.current_ϵ(stepsize_state)\n        # Reset the so far recorded intermediate and MCMC positions and gradients\n        reset!(recording_lpdf)\n        current_transition_counter = 0\n        steps_per_draw = current_steps(depth_predictor)\n        finish_cost = restart_cost = (n_draws + stepsize_adaptation_limit) * steps_per_draw\n        n_divergent = 0\n        n_divergent_samples = 0\n        current_evaluation_counter = 0\n        # We run the current warm-up/sampling window until \n        #   a) we have collected enough samples and can break out of the outer loop as well or\n        #   b) we have reached the current targeted number of gradient evaluations AND we estimate that \n        #       restarting (adding a new warm-up window) is better than finishing sampling with the current adaptation\n        while size(posterior_position, 2) &lt; n_draws && (\n            current_evaluation_counter &lt; n_evaluations || finish_cost &lt; restart_cost \n        )\n            current_transition_counter += 1\n            total_transition_counter += 1\n            # One MCMC transition\n            position_and_gradient, stats = DynamicHMC.sample_tree(rng, algorithm, hamiltonian, position_and_gradient, stepsize)\n            total_evaluation_counter += stats.steps\n            current_evaluation_counter += stats.steps\n            # Add information about the current transition to our predictor\n            record!(depth_predictor, stats)\n            is_divergent = DynamicHMC.is_divergent(stats.termination)\n            is_divergent && (n_divergent += 1)\n            if current_transition_counter &lt; stepsize_adaptation_limit\n                # The current warm-up window has seen fewer MCMC transitions than our step size adaptation limit.\n                # Continue adapting the step size.\n                stepsize_state = DynamicHMC.adapt_stepsize(stepsize_adaptation, stepsize_state, stats.acceptance_rate)\n                stepsize = DynamicHMC.current_ϵ(stepsize_state)\n            elseif current_transition_counter == stepsize_adaptation_limit\n                # The current warm-up window hits the step size adaptation limit.\n                # Finalize the stepsize.\n                stepsize = DynamicHMC.final_ϵ(stepsize_state)\n            else\n                # The current warm-up window has been sampling with the same linear transformation and step size.\n                # Record posterior positions, gradients and whether the current transition diverged\n                append!(posterior_position, position_and_gradient.q)\n                append!(posterior_gradient, position_and_gradient.∇ℓq)\n                is_divergent && (n_divergent_samples += 1)\n            end\n            # Update the current steps per draw and predicted steps per draw should we add a new warm-up window,\n            # as well as the estimated cost for finishing sampling with the current kernel or restarting warm-up. \n            # I think this actually only has to happen once, will change at some point in the future.\n            steps_per_draw = current_steps(depth_predictor)\n            potential_steps_per_draw = potential_steps(depth_predictor)\n            finish_cost = (n_draws + stepsize_adaptation_limit - current_transition_counter) * steps_per_draw\n            restart_cost = (n_draws + stepsize_adaptation_limit) * potential_steps_per_draw\n            report && ProgressMeter.update!(progress, size(posterior_position, 2), showvalues=pairs(\n                merge(\n                    round2((;outer_counter, current_transition_counter, total_transition_counter, transformation_losses, finish_cost, restart_cost, total_evaluation_counter, steps_per_draw, potential_steps_per_draw, stepsize, n_divergent, n_divergent_samples, min_ess, recorder=(;recorder.target, recorder.thin, recorder.outer_count, recorder.inner_count, recorder.triggered, recorded=size(halo_position, 2)))),\n                )\n            ))\n        end\n        advance!(depth_predictor)\n        # Double the targeted number of GRADIENT EVALUATIONS in the next warm-up window\n        n_evaluations *= 2\n        # Recompute the thinning factor for the intermediate positions and gradients\n        recorder.thin = n_evaluations ÷ recording_target\n        # Update the linear transformation candidates and estimate the transformation loss,\n        # using the INTERMEDIATE POSITIONS AND GRADIENTS.\n        # AT THIS POINT WE CAN ALSO LEARN NON-LINEAR TRANSFORMATIONS, \n        # AND AGAIN SELECT THE ONE THAT MINIMIZES THE TRANSFORMATION LOSS.\n        transformation_losses = map(L-&gt;update_loss!(L, (halo_position), (halo_gradient)), scale_options)\n        # Update the new linear transformation to be the one with the minimal estimated transformation loss.\n        active_option = argmin(transformation_losses)\n        kinetic_energy = energy_options[active_option]\n        (monitor_ess && size(posterior_position, 2) &gt; 10) && (min_ess = minimum(\n            MCMCDiagnosticTools.ess(reshape(posterior_position', (:, 1, dimension)))\n        ))\n    end\n    report && ProgressMeter.update!(progress, size(posterior_position, 2), showvalues=pairs(\n        merge(\n            round2((;outer_counter, total_transition_counter, transformation_losses, total_evaluation_counter, steps_per_draw, stepsize, n_divergent_samples, min_ess)),\n        )\n    ))\n    recording_lpdf\nend"
  },
  {
    "objectID": "index.html#sampling-efficiency-comparison-linear-transformations",
    "href": "index.html#sampling-efficiency-comparison-linear-transformations",
    "title": "Adaptive warm-up for linear and non-linear transformations",
    "section": "Sampling efficiency comparison: linear transformations",
    "text": "Sampling efficiency comparison: linear transformations\nThe below table compares the adaptive warmup procedure with the “regular” linear warm-up procedure as used e.g. by Stan, as implemented in DynamicHMC.jl. All benchmarks are run on my local machine using BridgeStan.jl and PosteriorDB.jl. This table only includes learning linear transformations.\nMeaning of columns:\n\nPosterior: The name of the posterior in posteriordb.\ndimension: The dimension of the posterior.\n#runs: The number of individual chains run to compute the benchmark statistics (the next columns).\nAdaptive advantage: The ratio of the mean sampling efficiency of the adaptive warm-up and the mean sampling efficiency of the regular warm-up. The chainwise sampling efficiency has been computed my dividing the minimum (over the parameters) effective sample size by the total number of gradient evaluations in that chain’s run.\nAdaptive/Regular: min. ess: The mean minimum effective sample size (for the mean) for the adaptive or regular warm-up respectively.\nAdaptive/Regular: #gradient evals: The mean total number of gradient evaluations.\n\nPosteriors where the regular warm-up takes too long have been skipped so far due to time-reasons. The table is sorted first by #runs (descending) and then by Adaptive advantage (ascending).\n\n\nPrecompiling WarmupHMC\n  ✓ WarmupHMC\n  1 dependency successfully precompiled in 4 seconds. 198 already precompiled.\n\n\n\n\n\n\n\n\nPosterior\ndimension\n#runs\nAdaptive advantage\nAdaptive: min. ess\nRegular: min. ess\nAdaptive: #gradient evals\nRegular: #gradient evals\n\n\n\n\nmcycle_splines-accel_splines\n82\n4\n0.091\n4.7\n110\n780k\n1.8M\n\n\nmcycle_gp-accel_gp\n66\n4\n0.18\n45\n430\n890k\n1.4M\n\n\nradon_mn-radon_variable_slope_centered\n89\n4\n0.2\n7.6\n53\n57k\n46k\n\n\nrats_data-rats_model\n65\n4\n0.5\n160\n600\n330k\n73k\n\n\nGLMM_Poisson_data-GLMM_Poisson_model\n45\n4\n0.7\n2.4\n21\n39k\n120k\n\n\nSurvey_data-Survey_model\n1\n4\n1.1\n170\n200\n6.9k\n9.2k\n\n\neight_schools-eight_schools_centered\n10\n4\n1.2\n27\n33\n37k\n52k\n\n\nRate_4_data-Rate_4_model\n2\n4\n1.3\n520\n620\n5.5k\n8.3k\n\n\ngp_pois_regr-gp_regr\n3\n4\n1.4\n900\n930\n6.2k\n8.9k\n\n\nRate_3_data-Rate_3_model\n1\n4\n1.4\n330\n340\n3.4k\n4.7k\n\n\nRate_1_data-Rate_1_model\n1\n4\n1.4\n370\n420\n3.2k\n5k\n\n\nnormal_2-normal_mixture\n3\n4\n1.4\n920\n930\n5.8k\n8.3k\n\n\nradon_mn-radon_partially_pooled_centered\n88\n4\n1.5\n220\n270\n17k\n31k\n\n\nMtbh_data-Mtbh_model\n154\n4\n1.5\n170\n190\n18k\n31k\n\n\nRate_2_data-Rate_2_model\n2\n4\n1.5\n750\n730\n4.9k\n7.1k\n\n\nradon_mn-radon_variable_intercept_centered\n89\n4\n1.6\n360\n420\n17k\n31k\n\n\nseeds_data-seeds_centered_model\n26\n4\n1.7\n88\n120\n16k\n37k\n\n\nscience_irt-grsm_latent_reg_irt\n408\n4\n1.7\n99\n210\n20k\n72k\n\n\nsoil_carbon-soil_incubation\n6\n4\n1.7\n26\n41\n230k\n160k\n\n\nsurgical_data-surgical_model\n14\n4\n1.8\n260\n270\n13k\n22k\n\n\nthree_men2-ldaK2\n510\n4\n1.8\n420\n410\n23k\n40k\n\n\nwells_data-wells_dae_inter_model\n7\n4\n1.8\n1.3k\n1.2k\n7.3k\n12k\n\n\nradon_mn-radon_variable_intercept_slope_noncentered\n175\n4\n1.8\n98\n220\n20k\n84k\n\n\ngp_pois_regr-gp_pois_regr\n13\n4\n1.8\n150\n390\n75k\n330k\n\n\narma-arma11\n4\n4\n1.8\n880\n830\n6.9k\n12k\n\n\nradon_mn-radon_hierarchical_intercept_centered\n90\n4\n1.9\n51\n59\n17k\n40k\n\n\nRate_5_data-Rate_5_model\n1\n4\n1.9\n350\n330\n2.8k\n4.8k\n\n\nnormal_5-normal_mixture_k\n14\n4\n1.9\n11\n11\n94k\n180k\n\n\nbones_data-bones_model\n13\n4\n1.9\n1.8k\n1.6k\n8.3k\n14k\n\n\nwells_data-wells_interaction_c_model\n4\n4\n2\n950\n780\n6.6k\n11k\n\n\nMb_data-Mb_model\n3\n4\n2\n630\n560\n5.9k\n11k\n\n\nuk_drivers-state_space_stochastic_level_stochastic_seasonal\n389\n4\n2\n7.1\n8.1\n770k\n1.3M\n\n\npilots-pilots\n18\n4\n2\n9.9\n13\n170k\n360k\n\n\nirt_2pl-irt_2pl\n144\n4\n2\n59\n66\n22k\n50k\n\n\nGLMM_data-GLMM1_model\n237\n4\n2.1\n1.6k\n1.3k\n19k\n32k\n\n\nthree_men1-ldaK2\n502\n4\n2.1\n360\n350\n19k\n40k\n\n\ntimssAusTwn_irt-gpcm_latent_reg_irt\n530\n4\n2.1\n360\n320\n37k\n70k\n\n\ndogs-dogs_nonhierarchical\n65\n4\n2.2\n190\n200\n19k\n42k\n\n\nradon_mn-radon_variable_intercept_slope_centered\n175\n4\n2.2\n14\n22\n20k\n74k\n\n\neight_schools-eight_schools_noncentered\n10\n4\n2.2\n500\n530\n8.4k\n20k\n\n\none_comp_mm_elim_abs-one_comp_mm_elim_abs\n4\n4\n2.3\n350\n320\n9.5k\n19k\n\n\nMt_data-Mt_model\n4\n4\n2.3\n910\n600\n6.5k\n9.8k\n\n\nthree_men3-ldaK2\n505\n4\n2.3\n420\n370\n21k\n42k\n\n\nM0_data-M0_model\n2\n4\n2.4\n850\n540\n5k\n7.7k\n\n\nradon_all-radon_pooled\n3\n4\n2.4\n910\n600\n5.8k\n9.1k\n\n\nradon_mn-radon_pooled\n3\n4\n2.4\n980\n690\n5.6k\n9.6k\n\n\nseeds_data-seeds_model\n26\n4\n2.5\n66\n70\n15k\n40k\n\n\nlow_dim_gauss_mix-low_dim_gauss_mix\n5\n4\n2.5\n1.1k\n880\n7.1k\n14k\n\n\nradon_mn-radon_county_intercept\n87\n4\n2.7\n960\n1000\n13k\n35k\n\n\nbutterfly-multi_occupancy\n106\n4\n2.7\n70\n80\n37k\n110k\n\n\nradon_mn-radon_partially_pooled_noncentered\n88\n4\n2.8\n270\n380\n15k\n60k\n\n\ngarch-garch11\n4\n4\n2.8\n470\n410\n8.4k\n21k\n\n\ndogs-dogs_hierarchical\n2\n4\n2.9\n710\n410\n5.3k\n8.7k\n\n\nradon_all-radon_variable_slope_centered\n390\n4\n3.2\n210\n280\n21k\n90k\n\n\nmesquite-logmesquite_logvolume\n3\n4\n3.2\n820\n490\n6.2k\n12k\n\n\nseeds_data-seeds_stanified_model\n26\n4\n3.2\n93\n66\n16k\n35k\n\n\nkidiq_with_mom_work-kidscore_interaction_c\n5\n4\n3.4\n1.1k\n780\n7k\n18k\n\n\nGLM_Binomial_data-GLM_Binomial_model\n3\n4\n3.5\n920\n580\n5.8k\n13k\n\n\nthree_docs1200-ldaK2\n7\n4\n3.5\n620\n470\n9.4k\n25k\n\n\nwells_data-wells_dae_c_model\n5\n4\n3.5\n860\n640\n6.5k\n17k\n\n\nradon_mn-radon_variable_intercept_noncentered\n89\n4\n3.6\n400\n350\n19k\n60k\n\n\ndogs-dogs_log\n2\n4\n3.7\n580\n280\n6.3k\n11k\n\n\nsesame_data-sesame_one_pred_a\n3\n4\n4.6\n860\n450\n6.1k\n15k\n\n\nrstan_downloads-prophet\n62\n4\n4.6\n150\n520\n210k\n1.6M\n\n\nradon_mn-radon_hierarchical_intercept_noncentered\n90\n4\n4.6\n300\n260\n19k\n79k\n\n\nloss_curves-losscurve_sislob\n15\n4\n4.8\n840\n450\n9k\n23k\n\n\nsynthetic_grid_RBF_kernels-kronecker_gp\n438\n4\n5\n86\n140\n300k\n1.9M\n\n\nradon_mn-radon_variable_slope_noncentered\n89\n4\n5.2\n230\n210\n16k\n75k\n\n\nwells_data-wells_daae_c_model\n6\n4\n5.5\n1.1k\n600\n7k\n20k\n\n\nradon_mod-radon_county\n389\n4\n5.9\n1.2k\n710\n18k\n67k\n\n\nkidiq_with_mom_work-kidscore_interaction_c2\n5\n4\n6\n1.1k\n510\n7k\n20k\n\n\nwells_data-wells_dae_model\n4\n4\n6\n940\n570\n6.7k\n25k\n\n\nwells_data-wells_dist100ars_model\n3\n4\n6.7\n890\n460\n5.6k\n19k\n\n\ndugongs_data-dugongs_model\n4\n4\n6.7\n480\n390\n7.2k\n39k\n\n\nwells_data-wells_dist100_model\n2\n4\n6.8\n800\n260\n5k\n11k\n\n\nearnings-logearn_interaction_z\n5\n4\n7.1\n1k\n490\n6.6k\n22k\n\n\ndogs-dogs\n3\n4\n7.5\n900\n420\n6.1k\n21k\n\n\nradon_all-radon_variable_slope_noncentered\n390\n4\n9.3\n86\n100\n20k\n230k\n\n\nGLM_Poisson_Data-GLM_Poisson_model\n4\n4\n12\n980\n330\n3.2k\n28k\n\n\nsblri-blr\n6\n4\n14\n1.1k\n200\n7.5k\n19k\n\n\nnes1992-nes\n10\n4\n15\n820\n440\n10k\n80k\n\n\nmesquite-logmesquite\n8\n4\n16\n870\n350\n8.4k\n54k\n\n\narK-arK\n7\n4\n18\n1.3k\n560\n8k\n59k\n\n\nnes1976-nes\n10\n4\n19\n860\n430\n8.9k\n85k\n\n\nnes_logit_data-nes_logit_model\n2\n4\n19\n950\n180\n4.8k\n18k\n\n\nmesquite-logmesquite_logva\n5\n4\n20\n950\n360\n7.3k\n56k\n\n\nhudson_lynx_hare-lotka_volterra\n8\n4\n21\n600\n230\n7.2k\n67k\n\n\nwells_data-wells_interaction_model\n4\n4\n22\n950\n210\n6.9k\n33k\n\n\nnes2000-nes\n10\n4\n23\n760\n340\n9k\n92k\n\n\nnes1972-nes\n10\n4\n24\n1.1k\n420\n8.7k\n79k\n\n\nnes1988-nes\n10\n4\n24\n1.1k\n440\n8.7k\n82k\n\n\nkidiq_with_mom_work-kidscore_interaction_z\n5\n4\n24\n1.1k\n400\n6.9k\n63k\n\n\nnes1984-nes\n10\n4\n25\n1.2k\n480\n8.5k\n83k\n\n\nkidiq-kidscore_momiq\n3\n4\n25\n830\n350\n6.1k\n66k\n\n\nsblrc-blr\n6\n4\n26\n1.1k\n180\n7.8k\n32k\n\n\nnes1996-nes\n10\n4\n26\n1k\n420\n8.4k\n91k\n\n\nwells_data-wells_dist\n2\n4\n30\n920\n190\n5k\n31k\n\n\nmesquite-logmesquite_logvas\n8\n4\n34\n1.1k\n380\n8k\n93k\n\n\nkidiq-kidscore_momhs\n3\n4\n37\n1.1k\n200\n5.3k\n36k\n\n\nmesquite-logmesquite_logvash\n7\n4\n38\n880\n250\n7.9k\n87k\n\n\nkidiq_with_mom_work-kidscore_mom_work\n5\n4\n39\n930\n260\n7.4k\n78k\n\n\nnes1980-nes\n10\n4\n40\n1.5k\n380\n8.3k\n85k\n\n\nlow_dim_gauss_mix_collapse-low_dim_gauss_mix_collapse\n5\n4\n42\n100\n9.2\n11k\n40k\n\n\nsir-sir\n4\n4\n47\n800\n310\n6.7k\n120k\n\n\nkidiq-kidscore_momhsiq\n4\n4\n67\n970\n320\n6.4k\n140k\n\n\nearnings-log10earn_height\n3\n4\n75\n970\n310\n6.1k\n150k\n\n\nearnings-logearn_height\n3\n4\n82\n960\n260\n5.6k\n120k\n\n\nearnings-logearn_height_male\n4\n4\n110\n990\n300\n6.6k\n210k\n\n\ndiamonds-diamonds\n26\n4\n110\n430\n330\n25k\n1.5M\n\n\nkidiq-kidscore_interaction\n5\n4\n130\n1.1k\n190\n7.5k\n170k\n\n\nmesquite-mesquite\n8\n4\n180\n870\n500\n11k\n1M\n\n\nearnings-logearn_interaction\n5\n4\n180\n1.1k\n280\n7.1k\n320k\n\n\nearnings-logearn_logheight_male\n4\n4\n520\n1.2k\n190\n5.9k\n460k\n\n\nkilpisjarvi_mod-kilpisjarvi\n3\n4\n1.1k\n830\n150\n6.1k\n1.3M\n\n\nearnings-earn_height\n3\n4\n2.2k\n980\n61\n5.9k\n800k\n\n\ntraffic_accident_nyc-bym2_offset_only\n3845\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nstate_wide_presidential_votes-hierarchical_gp\n933\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nsat-hier_2pl\n669\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_variable_intercept_slope_noncentered\n777\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_variable_intercept_slope_centered\n777\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_variable_intercept_noncentered\n390\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_variable_intercept_centered\n390\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_partially_pooled_noncentered\n389\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_partially_pooled_centered\n389\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_hierarchical_intercept_noncentered\n391\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_hierarchical_intercept_centered\n391\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nradon_all-radon_county_intercept\n388\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nprostate-logistic_regression_rhs\n11935\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nprideprejudice_paragraph-ldaK5\n15570\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nprideprejudice_chapter-ldaK5\n7714\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\novarian-logistic_regression_rhs\n3075\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nmnist_100-nn_rbm1bJ10\n7951\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nmnist-nn_rbm1bJ100\n79411\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nlsat_data-lsat_model\n1006\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\niohmm_reg_simulated-iohmm_reg\n29\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nhmm_gaussian_simulated-hmm_gaussian\n14\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nhmm_example-hmm_example\n4\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nfims_Aus_Jpn_irt-2pl_latent_reg_irt\n531\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nelection88-election88_full\n90\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\necdc0501-covid19imperial_v3\n51\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\necdc0501-covid19imperial_v2\n51\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\necdc0401-covid19imperial_v3\n51\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\necdc0401-covid19imperial_v2\n51\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nbball_drive_event_1-hmm_drive_1\n6\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nbball_drive_event_0-hmm_drive_0\n6\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nMth_data-Mth_model\n394\n0\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\nMh_data-Mh_model\n388\n0\nmissing\nmissing\nmissing\nmissing\nmissing"
  },
  {
    "objectID": "index.html#sampling-efficiency-comparison-non-linear-transformations",
    "href": "index.html#sampling-efficiency-comparison-non-linear-transformations",
    "title": "Adaptive warm-up for linear and non-linear transformations",
    "section": "Sampling efficiency comparison: non-linear transformations",
    "text": "Sampling efficiency comparison: non-linear transformations\nTBA"
  }
]